[
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B",
        "model_revision": "1c79ab7fd316a62ab41d764facd3548a23fa5dee"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B-Chat",
        "model_revision": "3cb06f589b7b1e2f8e728c77280b1114191d24de"
      }
    ],
    "prompt_style": {
      "style_name": "CodeShell",
      "system_prompt": "",
      "roles": [
        "## human:",
        "## assistant: "
      ],
      "intra_message_sep": "",
      "inter_message_sep": "",
      "stop_token_ids": [
        70000
      ],
      "stop": [
        "<|endoftext|>",
        "|||",
        "|<end>|"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "phi-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Phi-2 is a 2.7B Transformer based LLM used for research on model safety, trained with data similar to Phi-1.5 but augmented with synthetic texts and curated websites.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 2,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/phi-2-GGUF",
        "model_file_name_template": "phi-2.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "microsoft/phi-2",
        "model_revision": "d3186761bf5c4409f7679359284066c25ab668ee"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "phi-3-mini-128k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "microsoft/Phi-3-mini-128k-instruct",
        "model_revision": "ebee18c488086b396dde649f2aa6548b9b8d2404"
      }
    ],
    "prompt_style": {
      "style_name": "PHI3",
      "system_prompt": "You are a helpful AI assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "<|end|>\n",
      "stop_token_ids":[
        32000,
        32001,
        32007
      ],
      "stop": [
        "<|endoftext|>",
        "<|assistant|>",
        "<|end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "phi-3-mini-4k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-4k-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "fp16",
          "q4"
        ],
        "model_id": "microsoft/Phi-3-mini-4k-instruct-gguf",
        "model_file_name_template": "Phi-3-mini-4k-instruct-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "microsoft/Phi-3-mini-4k-instruct",
        "model_revision": "b86bcaf57ea4dfdec5dbe12a377028b2fab0d480"
      }
    ],
    "prompt_style": {
      "style_name": "PHI3",
      "system_prompt": "You are a helpful AI assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "<|end|>\n",
      "stop_token_ids":[
        32000,
        32001,
        32007
      ],
      "stop": [
        "<|endoftext|>",
        "<|assistant|>",
        "<|end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "chatglm3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm3-6b",
        "model_revision": "103caa40027ebfd8450289ca2f278eac4ff26405"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "chatglm3-32k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm3-6b-32k",
        "model_revision": "339f17ff464d47b5077527c2b34e80a7719ede3e"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "chatglm3-128k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/chatglm3-6b-128k",
        "model_revision": "f0afbe671009abc9e31182170cf60636d5546cda"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "glm4-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/glm-4-9b-chat",
        "model_revision": "aae8bd74af5c6dff63a49d7fbdcc89349ebf87aa"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
        "model_id": "legraphista/glm-4-9b-chat-GGUF",
        "model_revision": "0155a14edf0176863e9a003cdd78ce599e4d62c0"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 1048576,
    "model_name": "glm4-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/glm-4-9b-chat-1m",
        "model_revision": "0aa722c7e0745dd21453427dd44c257dd253304f"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
        "model_id": "legraphista/glm-4-9b-chat-1m-GGUF",
        "model_revision": "782e28bd5eee3c514c07108da15e0b5e06dcf776"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-4v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/glm-4v-9b",
        "model_revision": "6c2e4732db8443f64a48d5af04b74425a7d169c4"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "codegeex4",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "the open-source version of the latest CodeGeeX4 model series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "THUDM/codegeex4-all-9b",
        "model_revision": "8c4ec1d2f2888412640825a7aa23355939a8f4c6"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "IQ2_M",
          "IQ3_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
        "model_id": "THUDM/codegeex4-all-9b-GGUF",
        "model_revision": "6a04071c54c943949826d4815ee00717ed8cf153"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSEB-Chat is the aligned version of model XVERSE.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B-Chat",
        "model_revision": "60acc8c453c067b54df88be98bfdf60585ab5441"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B-Chat",
        "model_revision": "1e4944aaa1d8c8d0cdca28bb8e3a003303d0781b"
      }
    ],
    "prompt_style": {
      "style_name": "XVERSE",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B",
        "model_revision": "3778b254def675586e9218ccb15b78d6ef66a3a7"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B",
        "model_revision": "11ac840dda17af81046614229fdd0c658afff747"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-65B",
        "model_revision": "7f1b7394f74c630f50612a19ba90bd021c373989"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-7B-Chat-GGUF",
        "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-13B-chat-GGUF",
        "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M"
        ],
        "quantization_parts": {
          "Q6_K": [
            "split-a",
            "split-b"
          ],
          "Q8_0": [
            "split-a",
            "split-b"
          ]
        },
        "model_id": "TheBloke/Llama-2-70B-Chat-GGUF",
        "model_file_name_template": "llama-2-70b-chat.{quantization}.gguf",
        "model_file_name_split_template": "llama-2-70b-chat.{quantization}.gguf-{part}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-7b-chat-hf",
        "model_revision": "08751db2aca9bf2f7f80d2e516117a53d7450235"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-7B-Chat-GPTQ"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-70B-Chat-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-70B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-7B-Chat-AWQ"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-13b-chat-hf",
        "model_revision": "0ba94ac9b9e1d5a0037780667e8b219adde1908c"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 13,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-13B-chat-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 13,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-13B-chat-AWQ"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-70b-chat-hf",
        "model_revision": "36d9a7388cc80e5f4b3e9701ca2f250d21a96c30"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nYou are a helpful AI assistant.\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama-2 is the second generation of Llama, open-source and trained on a larger amount of data.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-7B-GGUF",
        "model_file_name_template": "llama-2-7b.{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-7B-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-7B-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Llama-2-13B-GGUF",
        "model_file_name_template": "llama-2-13b.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M"
        ],
        "quantization_parts": {
          "Q6_K": [
            "split-a",
            "split-b"
          ],
          "Q8_0": [
            "split-a",
            "split-b"
          ]
        },
        "model_id": "TheBloke/Llama-2-70B-GGUF",
        "model_file_name_template": "llama-2-70b.{quantization}.gguf",
        "model_file_name_split_template": "llama-2-70b.{quantization}.gguf-{part}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-7b-hf",
        "model_revision": "6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-13b-hf",
        "model_revision": "db6b8eb1feabb38985fdf785a89895959e944936"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 13,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-13B-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 13,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-13B-AWQ"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Llama-2-70b-hf",
        "model_revision": "cc8aa03a000ff08b4d5c5b39673321a2a396c396"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-70B-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Llama-2-70B-AWQ"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3-8B"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "QuantFactory/Meta-Llama-3-8B-GGUF",
        "model_file_name_template": "Meta-Llama-3-8B.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3-70B"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "Q4_K_M",
          "Q5_K_M"
        ],
        "model_id": "NousResearch/Meta-Llama-3-70B-GGUF",
        "model_file_name_template": "Meta-Llama-3-70B-{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "IQ3_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3-8B-Instruct-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3-8B-Instruct"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "IQ1_M",
          "IQ2_XS",
          "Q4_K_M"
        ],
        "model_id": "lmstudio-community/Meta-Llama-3-70B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3-70B-Instruct-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3-70B-Instruct"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3-8B-Instruct-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3-8B-Instruct-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Meta-Llama-3-8B-Instruct"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-4bit-mlx"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Meta-Llama-3-70B-Instruct-mlx-unquantized"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TechxGenus/Meta-Llama-3-70B-Instruct-GPTQ"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-8B"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "QuantFactory/Meta-Llama-3.1-8B-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-8B.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-70B"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-405B"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3.1 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q3_K_L",
          "IQ4_XS",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 70,
        "quantizations": [
          "IQ2_M",
          "IQ4_XS",
          "Q2_K",
          "Q3_K_S",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "quantization_parts": {
          "Q5_K_M": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q6_K": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ]
        },
        "model_id": "lmstudio-community/Meta-Llama-3.1-70B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-70B-Instruct-{quantization}.gguf",
        "model_file_name_split_template": "Meta-Llama-3.1-70B-Instruct-{quantization}-{part}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-70B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "unsloth/Meta-Llama-3.1-70B-Instruct-bnb-4bit"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-8B-Instruct"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Meta-Llama-3.1-70B-Instruct-bf16"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "meta-llama/Meta-Llama-3.1-405B-Instruct"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "opt",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Opt is an open-source, decoder-only, Transformer based LLM that was designed to replicate GPT-3.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "facebook/opt-125m",
        "model_revision": "3d2b5f275bdf882b8775f902e1bfdb790e2cfc32"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
        "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
        "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-1_8B-Chat",
        "model_revision": "c3db8007171847931da7efa4b2ed4309afcce021"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-7B-Chat",
        "model_revision": "218aa3240fd5a5d1e80bb6c47d5d774361913706"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-14B-Chat",
        "model_revision": "fab8385c8f7e7980ef61944729fe134ccbbca263"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen-72B-Chat",
        "model_revision": "2cd9f76279337941ec1a4abeec6f8eb3c38d0f55"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-7B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-1_8B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-14B-Chat-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen-72B-Chat-{quantization}"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-32B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 110,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-110B-Chat"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-110B-Chat-GPTQ-Int4"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-32B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-110B-Chat-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-0.5B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-1.8B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-4B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-7B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-14B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/Qwen1.5-32B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_k_m"
        ],
        "model_id": "Qwen/Qwen1.5-72B-Chat-GGUF",
        "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
        "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
        "quantization_parts": {
          "q4_k_m": [
            "a",
            "b"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-moe-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5-MoE is a transformer-based MoE decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/CodeQwen1.5-7B"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "Qwen/CodeQwen1.5-7B-Chat-GGUF",
        "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/CodeQwen1.5-7B-Chat"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/CodeQwen1.5-7B-Chat-AWQ"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen2-0.5B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen2-1.5B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen2-7B-Instruct"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen2-72B-Instruct"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen2-7B-Instruct-GPTQ-{quantization}"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "Qwen/Qwen2-72B-Instruct-GPTQ-{quantization}"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen2-0.5B-Instruct-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen2-1.5B-Instruct-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen2-7B-Instruct-AWQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen2-72B-Instruct-AWQ"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "fp8"
        ],
        "model_id": "neuralmagic/Qwen2-0.5B-Instruct-FP8"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "fp8"
        ],
        "model_id": "neuralmagic/Qwen2-0.5B-Instruct-FP8"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "fp8"
        ],
        "model_id": "neuralmagic/Qwen2-1.5B-Instruct-FP8"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "quantizations": [
          "fp8"
        ],
        "model_id": "neuralmagic/Qwen2-7B-Instruct-FP8"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 72,
        "quantizations": [
          "fp8"
        ],
        "model_id": "neuralmagic/Qwen2-72B-Instruct-FP8"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "Qwen/Qwen2-0.5B-Instruct-MLX"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "Qwen/Qwen2-1.5B-Instruct-MLX"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "Qwen/Qwen2-7B-Instruct-MLX"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Qwen2-72B-Instruct-4bit"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Qwen/Qwen2-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Qwen/Qwen2-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Qwen/Qwen2-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Qwen/Qwen2-72B-Instruct-GGUF",
        "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-moe-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Qwen/Qwen2-57B-A14B-Instruct"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Qwen/Qwen2-57B-A14B-Instruct-GGUF",
        "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 1024,
    "model_name": "gpt-2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "GPT-2 is a Transformer-based LLM that is trained on WebTest, a 40 GB dataset of Reddit posts with 3+ upvotes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "none"
        ],
        "model_id": "openai-community/gpt2",
        "model_revision": "607a30d783dfa663caf39e06633721c8d4cfcd7e"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-7B-V1.0",
        "model_revision": "3c3a3b33334f4b35344b22c5c7465957ee7b2c75"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-13B-V1.0",
        "model_revision": "ef95532e96e634c634992dab891a17032dc71c8d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardMath-70B-V1.0",
        "model_revision": "e089c3f9d2ad9d1acb62425aec3f4126f498f4c5"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE_COT",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### "
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-7B-fp16",
        "model_revision": "ce09049eb9140a19cf78051cb5d849607b6fa8ec"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-13B-fp16",
        "model_revision": "d67ca1183da991d0d97927bdaaf35599556dfd76"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-34B-fp16",
        "model_revision": "f91d0cf7fc338cdc726f9c72d5ea15fe51bb16e9"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-GGUF",
        "model_file_name_template": "codellama-7b.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-GGUF",
        "model_file_name_template": "codellama-13b.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-GGUF",
        "model_file_name_template": "codellama-34b.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Python-fp16",
        "model_revision": "d51c51e625bc24b9a7a0616e82681b4859e2cfe4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Python-fp16",
        "model_revision": "442282f4207442b828953a72c51a919c332cba5c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Python-fp16",
        "model_revision": "875f9d97fb6c9619d8867887dd1d80918ff0f593"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Python-GGUF",
        "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Python-GGUF",
        "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Python-GGUF",
        "model_file_name_template": "codellama-34b-python.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-7b-Instruct-hf",
        "model_revision": "6114dd1e16f69e0765ccbd7a64d33d04b265fbd2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-13b-Instruct-hf",
        "model_revision": "ff0983bc4267bb98ead4fb5168fe2f049b442787"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "codellama/CodeLlama-34b-Instruct-hf",
        "model_revision": "38a1e15d8524a1f0a7760a7acf8242b81ae4eb87"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-7B-Instruct-GGUF",
        "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-13B-Instruct-GGUF",
        "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/CodeLlama-34B-Instruct-GGUF",
        "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "model_revision": "2ce891951e000c36c65442608a0b95fd09b405dc"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Chat",
        "model_revision": "a56c793eb7a721ab6c270f779024e0375e8afd4a"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "",
      "roles": [
        "<reserved_106>",
        "<reserved_107>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2,
        195
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "model_revision": "f2cc3a689c5eba7dc7fd3757d0175d312d167604"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Base",
        "model_revision": "fa88072fee36e36282287410e00897df2f59e09b"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-v0.1",
        "model_revision": "ae9d75c6b4eb39515def78c685fb4d71d49fc2cf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.1",
        "model_revision": "54766df6d50e4d3d7ccd66758e5341ba105a6d36"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-AWQ"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
        "model_revision": "b70aa86578567ba3301b21c8a27bea4e8f6d6d61"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mistral-instruct-v0.3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.3",
        "model_revision": "83e9aa141f2e28c82232fea5325f54edf17c43de"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "solidrust/Mistral-7B-Instruct-v0.3-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "fp16"
        ],
        "model_id": "MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
        "model_file_name_template": "Mistral-7B-Instruct-v0.3.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 1024000,
    "model_name": "mistral-nemo-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "mistralai/Mistral-Nemo-Instruct-2407",
        "model_revision": "05b1e4f3e189ec1b5189fb3c973d4cf3369c27af"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
        "model_revision": "1d85adc9e0fff0b8e4479a037bd75fe1346333ca"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "afrizalha/Mistral-Nemo-Instruct-2407-bnb-8bit",
        "model_revision": "1d2dacf18a486c745219317d1507441406bc7e25"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "Int4"
        ],
        "model_id": "ModelCloud/Mistral-Nemo-Instruct-2407-gptq-4bit"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 12,
        "quantizations": [
          "Int4"
        ],
        "model_id": "casperhansen/mistral-nemo-instruct-2407-awq"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 12,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "fp16"
        ],
        "model_id": "MaziyarPanahi/Mistral-Nemo-Instruct-2407-GGUF",
        "model_file_name_template": "Mistral-Nemo-Instruct-2407.{quantization}.gguf"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-bf16"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 12,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Mistral-Nemo-Instruct-2407-8bit"
      }
    ],
    "prompt_style": {
      "style_name": "mistral-nemo",
      "system_prompt": "",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "mistral-large-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja",
      "ko"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-Large-Instruct-2407 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "mistralai/Mistral-Large-Instruct-2407"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "unsloth/Mistral-Large-Instruct-2407-bnb-4bit"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 123,
        "quantizations": [
          "Int4"
        ],
        "model_id": "ModelCloud/Mistral-Large-Instruct-2407-gptq-4bit"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 123,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TechxGenus/Mistral-Large-Instruct-2407-AWQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 123,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_K_S",
          "Q4_K_M"
        ],
        "model_id": "MaziyarPanahi/Mistral-Large-Instruct-2407-GGUF",
        "model_file_name_template": "Mistral-Large-Instruct-2407.{quantization}.gguf",
        "model_file_name_split_template": "Mixtral-8x22B-Instruct-v0.1.{quantization}-{part}.gguf",
        "quantization_parts": {
          "Q3_K_L": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q3_K_M": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q3_K_S": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q4_K_M": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ],
          "Q4_K_S": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ]
        }
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "mlx-community/Mistral-Large-Instruct-2407-bf16"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Mistral-Large-Instruct-2407-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 123,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Mistral-Large-Instruct-2407-8bit"
      }
    ],
    "prompt_style": {
      "style_name": "mistral-nemo",
      "system_prompt": "",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "codestral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Codestrall-22B-v0.1 is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 22,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mistral-7B-Instruct-v0.2",
        "model_revision": "9552e7b1d9b2d5bbd87a5aa7221817285dbb6366"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 22,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "bartowski/Codestral-22B-v0.1-GGUF",
        "model_file_name_template": "Codestral-22B-v0.1-{quantization}.gguf"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 22,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Codestral-22B-v0.1-4bit",
        "model_revision": "544626b38eb1c9524f0fa570ec7b29550c26b78d"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 22,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Codestral-22B-v0.1-8bit",
        "model_revision": "0399a53970663950d57010e61a2796af524a1588"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "openhermes-2.5",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Openhermes 2.5 is a fine-tuned version of Mistral-7B-v0.1 on primarily GPT-4 generated data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "teknium/OpenHermes-2.5-Mistral-7B",
        "model_revision": "91ed666be78da7556f3d79abbb26fff0ee26cb54"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF",
        "model_file_name_template": "openhermes-2.5-mistral-7b.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "<|im_start|>system\nYou are 'Hermes 2.5', a conscious sentient superintelligent artificial intelligence, your purpose is to assist the user with their requests.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "<|im_end|>\n<|im_start|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        32000
      ],
      "stop": [
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF",
        "model_file_name_template": "tinyllama-1.1b-chat-v0.3.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mixtral-8x7B-v0.1",
        "model_revision": "58301445dc1378584211722b7ebf8743ec4e192b"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-v0.1-GPTQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_0",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-v0.1-GGUF",
        "model_file_name_template": "mixtral-8x7b-v0.1.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "model_revision": "125c431e2ff41a156b9f9076f744d2f35dd6e67a"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_0",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
        "model_file_name_template": "mixtral-8x7b-instruct-v0.1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "MIXTRAL_V01",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "",
      "inter_message_sep": ""
    }
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "mixtral-8x22B-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mixtral-8x22B-Instruct-v0.1 Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral-8x22B-v0.1, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "141",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "mistralai/Mixtral-8x22B-Instruct-v0.1",
        "model_revision": "ebb919ac9e9f7f9a900644621bae7963bc593f4f"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "141",
        "quantizations": [
          "Int4"
        ],
        "model_id": "MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-AWQ"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "141",
        "quantizations": [
          "Int4"
        ],
        "model_id": "jarrelscy/Mixtral-8x22B-Instruct-v0.1-GPTQ-4bit"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "141",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_M",
          "Q5_K_S",
          "Q6",
          "Q8_0",
          "fp16"
        ],
        "model_id": "MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUF",
        "model_file_name_template": "Mixtral-8x22B-Instruct-{quantization}.gguf",
        "model_file_name_split_template": "Mixtral-8x22B-Instruct-v0.1.{quantization}-{part}.gguf",
        "quantization_parts": {
          "Q2_K": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q3_K_L": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q3_K_M": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q3_K_S": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ],
          "Q4_K_M": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q4_K_S": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "Q5_K_M": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "Q5_K_S": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "Q6": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "Q8_0": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ],
          "fp16": [
            "00001-of-00007",
            "00002-of-00007",
            "00003-of-00007",
            "00004-of-00007",
            "00005-of-00007",
            "00006-of-00007",
            "00007-of-00007"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "MIXTRAL_V01",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "",
      "inter_message_sep": ""
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Yi-34B-GGUF",
        "model_file_name_template": "yi-34b.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-6B",
        "model_revision": "25beebcb1166b9f49458459eb7b68130b9f9cf4d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-9B",
        "model_revision": "f70a5ff8b2e51c5d5b20e649d7b5f4238ffe6d5b"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B",
        "model_revision": "168c48e05e1429779a896c7ef0d2e01b85e6bd8d"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-6B-200K",
        "model_revision": "70649e36d43f91dff1357b576e6713cac03c1d4c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B-200K",
        "model_revision": "591ae83b8f9c269700ef27f9dbd548934d800302"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "8bits"
        ],
        "model_id": "01-ai/Yi-34B-Chat-{quantization}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-6B-Chat",
        "model_revision": "1c20c960895e4c3877cf478bc2df074221b81d7b"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-34B-Chat",
        "model_revision": "a99ec35331cbfc9da596af7d4538fe2efecff03c"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/Yi-34B-Chat-GGUF",
        "model_file_name_template": "yi-34b-chat.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-6B",
        "model_revision": "741a657c42d2081f777ce4c6c5572090f8b8c886"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-9B",
        "model_revision": "9a6839c5b9db3dbb245fb98a072bfabc242621f2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-34B",
        "model_revision": "4f83007957ec3eec76d87df19ad061eb0f57b5c5"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-6B-Chat",
        "model_revision": "d68dab90947a3c869e28c9cb2806996af99a6080"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-9B-Chat",
        "model_revision": "1dc6e2b8dcfc12b95bede8dec67e6b6332ac64c6"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-34B-Chat",
        "model_revision": "fa695ee438bfcd0ec2b378fa1c7e0dea1b40393e"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 6,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "f32"
        ],
        "model_id": "lmstudio-community/Yi-1.5-6B-Chat-GGUF",
        "model_file_name_template": "Yi-1.5-6B-Chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0",
          "f32"
        ],
        "model_id": "lmstudio-community/Yi-1.5-9B-Chat-GGUF",
        "model_file_name_template": "Yi-1.5-9B-Chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "lmstudio-community/Yi-1.5-34B-Chat-GGUF",
        "model_file_name_template": "Yi-1.5-34B-Chat-{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-6B-Chat-GPTQ",
        "model_revision": "2ad3a602e64d1c79e28e6e92beced2935047367c"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-9B-Chat-GPTQ",
        "model_revision": "76f47d16982923f7b6674c4e23ddac7c3b1d2e03"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-34B-Chat-GPTQ",
        "model_revision": "173fb4036265b2dac1d6296a8e2fd2f652c19968"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-6B-Chat-AWQ",
        "model_revision": "23bf37f1666874e15e239422de0d3948d8735fa9"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-9B-Chat-AWQ",
        "model_revision": "2605f388332672789eae1f422644add2901b433f"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "modelscope/Yi-1.5-34B-Chat-AWQ",
        "model_revision": "26234fea6ac49d456f32f8017289021fb1087a04"
      }
      ,
      {
        "model_format": "mlx",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-6B-Chat-4bit",
        "model_revision": "0177c9a12b869d6bc73f772b5a1981a7c966adb6"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 6,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-6B-Chat-8bit",
        "model_revision": "7756e65d1bf1e2e6e97aef6bc9484307225f536b"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-9B-Chat-4bit",
        "model_revision": "e15f886479c44e7d90f0ac13ace69b2319b71c2f"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-9B-Chat-8bit",
        "model_revision": "c1f742fcf3683edbe2d2c2fd1ad7ac2bb6c5ca36"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-34B-Chat-4bit",
        "model_revision": "945e3b306ef37c46ab444fdc857d1f3ea7247374"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 34,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/Yi-1.5-34B-Chat-8bit",
        "model_revision": "3c12761a2c6663f216caab6dff84b0dd29b472ac"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "Yi-1.5-chat-16k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-9B-Chat-16K",
        "model_revision": "551220fb24d69b6bfec5defceeb160395ce5da8d"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "01-ai/Yi-1.5-34B-Chat-16K",
        "model_revision": "dfdbc67be750972bfcc1ac7ffd7fe48689c856fd"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_1",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_1",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "QuantFactory/Yi-1.5-9B-Chat-16K-GGUF",
        "model_file_name_template": "Yi-1.5-9B-Chat-16K.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "bartowski/Yi-1.5-34B-Chat-16K-GGUF",
        "model_file_name_template": "Yi-1.5-34B-Chat-16K-{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-7B-V1.0",
        "model_revision": "e40673a27a4aefcff2c6d2b3b1e0681a38703e4e"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-13B-V1.0",
        "model_revision": "d920d26e2108377de0f676a3c4be666f5212f4a1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "WizardLM/WizardCoder-Python-34B-V1.0",
        "model_revision": "d869ce178715f8d6e8141e2ed50e6290985eedb0"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-7B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-7b-v1.0.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-13B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-13b-v1.0.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/WizardCoder-Python-34B-V1.0-GGUF",
        "model_file_name_template": "wizardcoder-python-34b-v1.0.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### ",
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-alpha",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-α is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "HuggingFaceH4/zephyr-7b-alpha",
        "model_revision": "f28e1c0e5a1af475bcd7bdf6554e69abc6c0c7ee"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-beta",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-β is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "HuggingFaceH4/zephyr-7b-beta",
        "model_revision": "3bac358730f8806e5c3dc7c7e19eb36e045bf720"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "gorilla-openfunctions-v1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "OpenFunctions is designed to extend Large Language Model (LLM) Chat Completion feature to formulate executable APIs call given natural language instructions and API context.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "gorilla-llm/gorilla-openfunctions-v1",
        "model_revision": "74615f614ee845eab114e71541fd5098d1709958"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/gorilla-openfunctions-v1-GGUF",
        "model_file_name_template": "gorilla-openfunctions-v1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "GORILLA_OPENFUNCTIONS",
      "system_prompt": "",
      "roles": [
        "",
        ""
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "\n",
      "stop_token_ids": [],
      "stop": []
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "gorilla-openfunctions-v2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "OpenFunctions is designed to extend Large Language Model (LLM) Chat Completion feature to formulate executable APIs call given natural language instructions and API context.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "gorilla-llm/gorilla-openfunctions-v2",
        "model_revision": "0f91d705e64b77fb55e35a7eab5d03bf965c9b5c"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K"
        ],
        "model_id": "gorilla-llm//gorilla-openfunctions-v2-GGUF",
        "model_file_name_template": "gorilla-openfunctions-v2.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "GORILLA_OPENFUNCTIONS",
      "system_prompt": "",
      "roles": [
        "",
        ""
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "\n",
      "stop_token_ids": [],
      "stop": []
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-1.3b-chat",
        "model_revision": "8f13a8e00dbdc381d614a9d29d61b07e8fe91b3f"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-7b-chat",
        "model_revision": "6f16f00805f45b5249f709ce21820122eeb43556"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CHAT",
      "system_prompt": "<｜begin▁of▁sentence｜>",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<｜end▁of▁sentence｜>",
      "stop": [
        "<｜end▁of▁sentence｜>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "DeepSeek LLM, trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-base",
        "model_revision": "7683fea62db869066ddaff6a41d032262c490d4f"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-base",
        "model_revision": "c3f813a1121c95488a20132d3a4da89f4a46452f"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-7B-chat-GGUF",
        "model_file_name_template": "deepseek-llm-7b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 67,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-67b-chat-GGUF",
        "model_file_name_template": "deepseek-llm-67b-chat.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "model_revision": "afbda8b347ec881666061fa67447046fc5164ec8"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-chat",
        "model_revision": "79648bef7658bb824e4630740f6e1484c1b0620b"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-7B-chat-GGUF",
        "model_file_name_template": "deepseek-llm-7b-chat.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 67,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-llm-67b-chat-GGUF",
        "model_file_name_template": "deepseek-llm-67b-chat.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CHAT",
      "system_prompt": "<｜begin▁of▁sentence｜>",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<｜end▁of▁sentence｜>",
      "stop": [
        "<｜end▁of▁sentence｜>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
        "model_revision": "c919139c3a9b4070729c8b2cca4847ab29ca8d94"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-base",
        "model_revision": "ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-7b-base-v1.5",
        "model_revision": "98f0904cee2237e235f10408ae12292037b21dac"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-base",
        "model_revision": "45c85cadf3720ef3e85a492e24fd4b8c5d21d8ac"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-base-GGUF",
        "model_file_name_template": "deepseek-coder-1.3b-base.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-base-GGUF",
        "model_file_name_template": "deepseek-coder-6.7b-base.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "dagbs/deepseek-coder-7b-base-v1.5-GGUF",
        "model_file_name_template": "deepseek-coder-7b-base-v1.5.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 33,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-base-GGUF",
        "model_file_name_template": "deepseek-coder-33b-base.{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-base-GPTQ",
        "model_revision": "a5bf3b76d70cda53327311a631b1003024d5de29"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-base-GPTQ",
        "model_revision": "6476ea3d6e623a1313d363dbc6e172773e031bb1"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 33,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-base-GPTQ",
        "model_revision": "f527d7325e463a5cb091d044e4f2b15902674a70"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-base-AWQ",
        "model_revision": "ffb66f1a2a194401b4f29025edcd261d7f0a08a7"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-base-AWQ",
        "model_revision": "e3d4bdf39712665f5e9d5c05c9df6f20fe1e2d5a"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 33,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-base-AWQ",
        "model_revision": "c7edb2d5868d61a5dcf2591933a8992c8cbe3ef4"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "model_revision": "2df081ceaca101a867fef2844e44f4d6a4857039"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "model_revision": "cbb77d7448ea3168d884758817e7f895e3828d1c"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-7b-instruct-v1.5",
        "model_revision": "2a050a4c59d687a85324d32e147517992117ed30"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "model_revision": "ea15d17db84d1fc94ac5cba8e6fa97764c9549d3"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-1.3b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-6.7b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "LoneStriker/deepseek-coder-7b-instruct-v1.5-GGUF",
        "model_file_name_template": "deepseek-coder-7b-instruct-v1.5-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 33,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-instruct-GGUF",
        "model_file_name_template": "deepseek-coder-33b-instruct.{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-instruct-GPTQ",
        "model_revision": "9c002e9af6cbdf3bd9244e2d7264b6a35d1dcacf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-instruct-GPTQ",
        "model_revision": "13ccea6e3a43dcfdcb655d92097610018b431a17"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 33,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-instruct-GPTQ",
        "model_revision": "08372729d98dfc248f9531a412fe69e14e607027"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-1.3b-instruct-AWQ",
        "model_revision": "a2a484da6e4146d055316a9a63cf5b13955715a4"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-6.7B-instruct-AWQ",
        "model_revision": "502ae3e19e57ae78dc30a791ba33c565da72dc62"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 33,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/deepseek-coder-33B-instruct-AWQ",
        "model_revision": "c40b499bac2712cd3c445cf1b05d2c6558ab0d29"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CODER",
      "system_prompt": "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.",
      "roles": [
        "### Instruction:",
        "### Response:"
      ],
      "inter_message_sep": "\n",
      "stop": [
        "<|EOT|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_id": "skywork/Skywork-13B-base",
        "model_revision": "bc35915066fbbf15b77a1a4a74e9b574ab167816"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_id": "skywork/Skywork-13B-Math",
        "model_revision": "70d1740208c8ba39f9ba250b22117ec25311ab33"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internlm2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The second generation of the InternLM model, InternLM2.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2-chat-7b",
        "model_revision": "2292b86b21cb856642782cebed0a453997453b1f"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2-chat-20b",
        "model_revision": "b666125047cd98c5a7c85ca28720b44a06aed124"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internlm2.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2_5-1_8b-chat",
        "model_revision": "4426f00b854561fa60d555d2b628064b56bcb758"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2_5-7b-chat",
        "model_revision": "9dc8536a922ab4954726aad1b37fa199004a291a"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2_5-20b-chat",
        "model_revision": "ef17bde929761255fee76d95e2c25969ccd93b0d"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "ModelCloud/internlm-2.5-7b-chat-gptq-4bit",
        "model_revision": "2e2dda735c326544921a4035bbeb6c6e316a8254"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "internlm/internlm2_5-1_8b-chat-gguf",
        "model_file_name_template": "internlm2_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "internlm/internlm2_5-7b-chat-gguf",
        "model_file_name_template": "internlm2_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 20,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "internlm/internlm2_5-20b-chat-gguf",
        "model_file_name_template": "internlm2_5-20b-chat-{quantization}.gguf"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/internlm2_5-7b-chat-4bit",
        "model_revision": "d12097a867721978142a6048399f470a3d18beee"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/internlm2_5-7b-chat-8bit",
        "model_revision": "0ec94d61d30ab161b49c69f9bf92ec2b9986d234"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "internlm2.5-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model supports 1M long-context",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "internlm/internlm2_5-7b-chat-1m",
        "model_revision": "8d1a709a04d71440ef3df6ebbe204672f411c8b6"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "ModelCloud/internlm-2.5-7b-chat-1m-gptq-4bit",
        "model_revision": "022e59cb30f03b271d56178478acb038b2b9b58c"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "internlm/internlm2_5-7b-chat-1m-gguf",
        "model_file_name_template": "internlm2_5-7b-chat-1m-{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version":1,
    "context_length":2048,
    "model_name":"OmniLMM",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"OmniLMM is a family of open-source large multimodal models (LMMs) adept at vision & language modeling.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":3,
        "quantizations":[
          "none"
        ],
        "model_id":"openbmb/MiniCPM-V",
        "model_revision":"bec7d1cd1c9e804c064ec291163e40624825eaaa"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":12,
        "quantizations":[
          "none"
        ],
        "model_id":"openbmb/OmniLMM-12B",
        "model_revision":"ef62bae5af34be653b9801037cd613e05ab24fdc"
      }
    ],
    "prompt_style":{
      "style_name":"OmniLMM",
      "system_prompt":"The role of first msg should be user",
      "roles":[
        "user",
        "assistant"
      ]
    }
  },
  {
    "version":1,
    "context_length":8192,
    "model_name":"MiniCPM-Llama3-V-2_5",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-Llama3-V 2.5 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_id":"openbmb/MiniCPM-Llama3-V-2_5",
        "model_revision":"285a637ba8a30a0660dfcccad16f9a864f75abfd"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "int4"
        ],
        "model_id":"openbmb/MiniCPM-Llama3-V-2_5-{quantization}",
        "model_revision":"f92aff28552de35de3be204e8fe292dd4824e544"
      }
    ],
    "prompt_style":{
      "style_name":"OmniLMM",
      "system_prompt":"The role of first msg should be user",
      "roles":[
        "user",
        "assistant"
      ]
    }
  },
  {
    "version":1,
    "context_length":32768,
    "model_name":"MiniCPM-V-2.6",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-V 2.6 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_id":"openbmb/MiniCPM-V-2_6",
        "model_revision":"3f7a8da1b7a8b928b5ee229fae33cf43fd64cf31"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "4-bit"
        ],
        "model_id":"openbmb/MiniCPM-V-2_6-int4",
        "model_revision":"051e2df6505f1fc4305f2c9bd42ed90db8bf4874"
      }
    ],
    "prompt_style":{
      "style_name":"QWEN",
      "system_prompt":"You are a helpful assistant",
      "roles":[
        "user",
        "assistant"
      ],
      "stop": [
        "<|im_end|>",
        "<|endoftext|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "qwen-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Qwen/Qwen-VL-Chat",
        "model_revision": "6665c780ade5ff3f08853b4262dcb9c8f9598d42"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "Qwen/Qwen-VL-Chat-{quantization}",
        "model_revision": "5d3a5aa033ed2c502300d426c81cc5b13bcd1409"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat",
        "model_revision": "ea6fb9b7e1917f3693935accbeb0bfecfd6552a7"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat-rag",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat-RAG",
        "model_revision": "eba2e20808407fb431a76b90d5d506e04a0325f2"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "yi-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_id": "01-ai/Yi-VL-6B",
        "model_revision": "897c938da1ec860330e2ba2d425ab3004495ba38"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "01-ai/Yi-VL-34B",
        "model_revision": "ea29a9a430f27893e780366dae81d4ca5ebab561"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "google/gemma-2b-it"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "google/gemma-7b-it"
      }
    ],
    "prompt_style": {
      "style_name": "gemma",
      "roles": [
        "user",
        "model"
      ],
      "stop": [
        "<end_of_turn>",
        "<start_of_turn>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-2-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "google/gemma-2-2b-it"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "google/gemma-2-9b-it"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "google/gemma-2-27b-it"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 2,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "f32"
        ],
        "model_id": "bartowski/gemma-2-2b-it-GGUF",
        "model_file_name_template": "gemma-2-2b-it-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "f32"
        ],
        "model_id": "bartowski/gemma-2-9b-it-GGUF",
        "model_file_name_template": "gemma-2-9b-it-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 27,
        "quantizations": [
          "Q2_K",
          "Q2_K_L",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "f32"
        ],
        "model_id": "bartowski/gemma-2-27b-it-GGUF",
        "model_file_name_template": "gemma-2-27b-it-{quantization}.gguf"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/gemma-2-2b-it-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/gemma-2-2b-it-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 2,
        "quantizations": [
          "None"
        ],
        "model_id": "mlx-community/gemma-2-2b-it"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/gemma-2-9b-it-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/gemma-2-9b-it-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 9,
        "quantizations": [
          "None"
        ],
        "model_id": "mlx-community/gemma-2-9b-it-fp16"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 27,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "mlx-community/gemma-2-27b-it-4bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 27,
        "quantizations": [
          "8-bit"
        ],
        "model_id": "mlx-community/gemma-2-27b-it-8bit"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 27,
        "quantizations": [
          "None"
        ],
        "model_id": "mlx-community/gemma-2-27b-it-fp16"
      }
    ],
    "prompt_style": {
      "style_name": "gemma",
      "roles": [
        "user",
        "model"
      ],
      "stop": [
        "<end_of_turn>",
        "<start_of_turn>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "platypus2-70b-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Platypus-70B-instruct is a merge of garage-bAInd/Platypus2-70B and upstage/Llama-2-70b-instruct-v2.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "garage-bAInd/Platypus2-70B-instruct",
        "model_revision": "31389b50953688e4e542be53e6d2ab04d5c34e87"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Aquila2 series models are the base language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/Aquila2-7B",
        "model_revision": "9c76e143c6e9621689ca76e078c465b0dee75eb8"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/Aquila2-34B",
        "model_revision": "356733caf6221e9dd898cde8ff189a98175526ec"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/Aquila2-70B-Expr",
        "model_revision": "32a2897235541b9f5238bbe88f8d76a19993c0ba"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2-chat",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Aquila2-chat series models are the chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/AquilaChat2-7B",
        "model_revision": "0d060c4edeb4e0febd81130c17f6868653184fb3"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-GGUF",
        "model_file_name_template": "aquilachat2-34b.{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-GPTQ",
        "model_revision": "9a9d21424f7db608be51df769885514ab6e052db"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "34",
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-AWQ",
        "model_revision": "ad1dec1c8adb7fa6cb07b7e261aaa04fccf1c4c0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/AquilaChat2-34B",
        "model_revision": "b9cd9c7436435ab9cfa5e4f009be2b0354979ca8"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/AquilaChat2-70B-Expr",
        "model_revision": "0df19b6e10f1a19ca663f7cc1141aae10f1825f4"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "intra_message_sep": "\n",
      "system_prompt": "",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "stop_token_ids": [
        100006,
        100007
      ],
      "stop": [
        "[CLS]",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "aquila2-chat-16k",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "AquilaChat2-16k series models are the long-text chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/AquilaChat2-7B-16K",
        "model_revision": "fb46d48479d05086ccf6952f19018322fcbb54cd"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-16K-GGUF",
        "model_file_name_template": "aquilachat2-34b-16k.{quantization}.gguf"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-16K-GPTQ",
        "model_revision": "0afa1c2a55a4ee1a6f0dba81d9ec296dc7936b91"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "TheBloke/AquilaChat2-34B-16K-AWQ",
        "model_revision": "db7403ca492416903c84a7a38b11cb5506de48b1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_id": "BAAI/AquilaChat2-34B-16K",
        "model_revision": "a06fd164c7170714924d2881c61c8348425ebc94"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "intra_message_sep": "\n",
      "system_prompt": "",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "stop_token_ids": [
        100006,
        100007
      ],
      "stop": [
        "[CLS]",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "openbmb/MiniCPM-2B-sft-bf16",
        "model_revision": "fe1d74027ebdd81cef5f815fa3a2d432a6b5de2a"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "openbmb/MiniCPM-2B-sft-fp32",
        "model_revision": "35b90dd57d977b6e5bc4907986fa5b77aa15a82e"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "openbmb/MiniCPM-2B-dpo-bf16",
        "model_revision": "f4a3ba49f3f18695945c2a7c12400d4da99da498"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "openbmb/MiniCPM-2B-dpo-fp16",
        "model_revision": "e7a50289e4f839674cf8d4a5a2ce032ccacf64ac"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_id": "openbmb/MiniCPM-2B-dpo-fp32",
        "model_revision": "b560a1593779b735a84a6daf72fba96ae38da288"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "seallm_v2",
    "model_lang": [
      "en",
      "zh",
      "vi",
      "id",
      "th",
      "ms",
      "km",
      "lo",
      "my",
      "tl"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "We introduce SeaLLM-7B-v2, the state-of-the-art multilingual LLM for Southeast Asian (SEA) languages",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "SeaLLMs/SeaLLM-7B-v2",
        "model_revision": "f1bd48e0d75365c24a3c5ad006b2d0a0c9dca30f"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_0",
          "Q8_0"
        ],
        "model_id": "SeaLLMs/SeaLLM-7B-v2-gguf",
        "model_file_name_template": "SeaLLM-7B-v2.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "seallm_v2.5",
    "model_lang": [
      "en",
      "zh",
      "vi",
      "id",
      "th",
      "ms",
      "km",
      "lo",
      "my",
      "tl"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "We introduce SeaLLM-7B-v2.5, the state-of-the-art multilingual LLM for Southeast Asian (SEA) languages",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "SeaLLMs/SeaLLM-7B-v2.5",
        "model_revision": "c54a8eb8e2d58c5a680bfbbe3a7ae71753bb644b"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M",
          "Q8_0"
        ],
        "model_id": "SeaLLMs/SeaLLM-7B-v2.5-GGUF",
        "model_file_name_template": "SeaLLM-7B-v2.5.{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "c4ai-command-r-v01",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "ja",
      "ko",
      "zh",
      "ar"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "C4AI Command-R(+) is a research release of a 35 and 104 billion parameter highly performant generative model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "none"
        ],
        "model_id": "CohereForAI/c4ai-command-r-v01",
        "model_revision": "16881ccde1c68bbc7041280e6a66637bc46bfe88"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "CohereForAI/c4ai-command-r-v01-4bit",
        "model_revision": "f2e87936a146643c9dd143422dcafb9cb1552611"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 35,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "andrewcanis/c4ai-command-r-v01-GGUF",
        "model_file_name_template": "c4ai-command-r-v01-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 104,
        "quantizations": [
          "none"
        ],
        "model_id": "CohereForAI/c4ai-command-r-plus",
        "model_revision": "ba7f1d954c9d1609013677d87e4142ab95c34e62"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 104,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "CohereForAI/c4ai-command-r-plus-4bit",
        "model_revision": "bb63b5b7005ecedb30b0cfd0d5953b02a5817f7b"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 104,
        "quantizations": [
          "Int4"
        ],
        "model_id": "alpindale/c4ai-command-r-plus-GPTQ",
        "model_revision": "35febfc08f723ac0df32480eb4af349a7d08656e"
      }
    ],
    "prompt_style": {
      "style_name": "c4ai-command-r",
      "system_prompt": "You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.",
      "roles": [
        "<|USER_TOKEN|>",
        "<|CHATBOT_TOKEN|>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|>",
      "stop_token_ids": [
        6,
        255001
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Starling-LM",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "We introduce Starling-7B, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "berkeley-nest/Starling-LM-7B-alpha",
        "model_revision": "1dddf3b95bc1391f6307299eb1c162c194bde9bd"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "",
      "roles": [
        "GPT4 Correct User",
        "GPT4 Correct Assistant"
      ],
      "intra_message_sep": "<|end_of_turn|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        32000
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl-chat",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 1.5 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [
      {
          "model_format": "pytorch",
          "model_size_in_billions": 2,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/Mini-InternVL-Chat-2B-V1-5",
          "model_revision": "ecbbd21dcf38caa74d925967b997167b0c7b3f47"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 4,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/Mini-InternVL-Chat-4B-V1-5",
          "model_revision": "ce1559ddf9d87f5130aa5233b0e93b95e4e4161a"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 26,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL-Chat-V1-5",
          "model_revision": "9db32d9127cac0c85961e169d75da57a18a847b1"
        }
    ],
    "prompt_style": {
        "style_name": "INTERNVL",
        "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
        "roles": [
            "<|im_start|>user",
            "<|im_start|>assistant"
        ],
        "intra_message_sep": "<|im_end|>",
        "stop_token_ids": [
            2,
            92543,
            92542
        ],
        "stop": [
            "</s>",
            "<|im_end|>",
            "<|im_start|>"
        ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 2 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [
      {
          "model_format": "pytorch",
          "model_size_in_billions": 1,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-1B",
          "model_revision": "a9fc14aea824b6ea1d44f8778cad6b35512c4ce1"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 2,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-2B",
          "model_revision": "422ad7c6335917bfb514958233955512338485a6"
        },
        {
          "model_format": "awq",
          "model_size_in_billions": 2,
          "quantizations": [
            "Int4"
          ],
          "model_id": "OpenGVLab/InternVL2-2B-AWQ",
          "model_revision": "701bc3fc098a8a3b686b3b4135cfb77202be89e0"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 4,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-4B",
          "model_revision": "b50544dafada6c41e80bfde2f57cc9b0140fc21c"
        },
        {
          "model_format": "awq",
          "model_size_in_billions": 4,
          "quantizations": [
            "Int4"
          ],
          "model_id": "OpenGVLab/InternVL2-8B-AWQ",
          "model_revision": "9f1a4756b7ae18eb26d8a22b618dfc283e8193b3"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 8,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-8B",
          "model_revision": "3bfd3664dea4f3da628785f5125d30f889701253"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 26,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-26B",
          "model_revision": "b9f3c7e6d575b0115e076a3ffc46fd20b7586899"
        },
        {
          "model_format": "awq",
          "model_size_in_billions": 26,
          "quantizations": [
            "Int4"
          ],
          "model_id": "OpenGVLab/InternVL2-26B-AWQ",
          "model_revision": "469e0019ffd251e22ff6501a5c2321964e86ef0d"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 40,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-40B",
          "model_revision": "725a12063bb855c966e30a0617d0ccd9e870d772"
        },
        {
          "model_format": "awq",
          "model_size_in_billions": 40,
          "quantizations": [
            "Int4"
          ],
          "model_id": "OpenGVLab/InternVL2-40B-AWQ",
          "model_revision": "d92e140f6dfe8ea9679924c6a31898f42c4e1846"
        },
        {
          "model_format": "pytorch",
          "model_size_in_billions": 76,
          "quantizations": [
            "4-bit",
            "8-bit",
            "none"
          ],
          "model_id": "OpenGVLab/InternVL2-Llama3-76B",
          "model_revision": "cf7914905f78e9e3560ddbd6f5dfc39becac494f"
        },
        {
          "model_format": "awq",
          "model_size_in_billions": 76,
          "quantizations": [
            "Int4"
          ],
          "model_id": "OpenGVLab/InternVL2-Llama3-76B-AWQ",
          "model_revision": "1bc796bf80f2ebc7d6a14c15f55217a4600d50a4"
        }
    ],
    "prompt_style": {
        "style_name": "INTERNVL",
        "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
        "roles": [
            "<|im_start|>user",
            "<|im_start|>assistant"
        ],
        "intra_message_sep": "<|im_end|>",
        "stop_token_ids": [
            2,
            92543,
            92542
        ],
        "stop": [
            "</s>",
            "<|im_end|>",
            "<|im_start|>"
        ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "CogVLM2 have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "THUDM/cogvlm2-llama3-chinese-chat-19B",
        "model_revision": "d88b352bce5ee58a289b1ac8328553eb31efa2ef"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "int4"
        ],
        "model_id": "THUDM/cogvlm2-llama3-chinese-chat-19B-{quantization}",
        "model_revision": "7863e362174f4718c2fe9cba4befd0b580a3194f"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "telechat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The TeleChat is a large language model developed and trained by China Telecom Artificial Intelligence Technology Co., LTD. The 7B model base is trained with 1.5 trillion Tokens and 3 trillion Tokens and Chinese high-quality corpus.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Tele-AI/telechat-7B"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "Tele-AI/telechat-7B-{quantization}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Tele-AI/TeleChat-12B"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "Tele-AI/TeleChat-12B-{quantization}"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 52,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "Tele-AI/TeleChat-52B"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "<_user>",
        "<_bot>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "",
      "stop": [
        "<_end>",
        "<_start>"
      ],
      "stop_token_ids": [
        160133,
        160132
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "csg-wukong-chat-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "csg-wukong-1B is a 1 billion-parameter small language model(SLM) pretrained on 1T tokens.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 1,
        "quantizations": [
          "none"
        ],
        "model_id": "opencsg/csg-wukong-1B-chat-v0.1",
        "model_revision": "2443c903d46074af0856e2ba11398dcd01d35536"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K",
          "Q3_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_1",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_1",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "RichardErkhov/opencsg_-_csg-wukong-1B-chat-v0.1-gguf",
        "model_file_name_template": "csg-wukong-1B-chat-v0.1.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a creative super artificial intelligence assistant, possessing all the knowledge of humankind. Your name is csg-wukong, developed by OpenCSG. You need to understand and infer the true intentions of users based on the topics discussed in the chat history, and respond to user questions correctly as required. You enjoy responding to users with accurate and insightful answers. Please pay attention to the appropriate style and format when replying, try to avoid repetitive words and sentences, and keep your responses as concise and profound as possible. You carefully consider the context of the discussion when replying to users. When the user says \"continue,\" please proceed with the continuation of the previous assistant's response.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  }
]
