[
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "llama-2-chat",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Llama-2-Chat is a fine-tuned version of the Llama-2 LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-7b-Chat-GGUF",
        "model_file_name_template": "llama-2-7b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_id": "Xorbits/Llama-2-13b-Chat-GGUF",
        "model_file_name_template": "llama-2-13b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-7b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.5"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-13b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "modelscope/Llama-2-70b-chat-ms",
        "model_hub": "modelscope",
        "model_revision": "v1.0.1"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nYou are a helpful AI assistant.\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "llama-3-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-8B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "swift/Meta-Llama-3-70B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "llama-3.1-instruct",
    "model_lang": [
      "en",
      "de",
      "fr",
      "it",
      "pt",
      "hi",
      "es",
      "th"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Llama 3.1 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks..",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 8,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 8,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 8,
        "quantizations": [
          "Q3_K_L",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-8B-Instruct-GGUF",
        "model_file_name_template": "Meta-Llama-3.1-8B-Instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 70,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-70B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 405,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-AWQ-INT4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 405,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "tiny-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 1,
        "quantizations": [
          "Q2_K"
        ],
        "model_id": "Xorbits/TinyLlama-1.1B-step-50K-105b-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v0.0.1",
        "model_file_name_template": "ggml-model-{quantization}.gguf"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Baichuan2-chat is a fine-tuned version of the Baichuan LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.4"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.3"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "",
      "roles": [
        "<reserved_106>",
        "<reserved_107>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2,
        195
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "baichuan-2",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Baichuan2 is an open-source Transformer based LLM that is trained on both Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-7B-Base",
        "model_revision": "v1.0.2",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "baichuan-inc/Baichuan2-13B-Base",
        "model_revision": "v1.0.3",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "chatglm3",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/chatglm3-6b",
        "model_revision": "v1.0.2"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "chatglm3-32k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/chatglm3-6b-32k",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "chatglm3-128k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "ChatGLM3 is the third generation of ChatGLM, still open-source and trained on Chinese and English data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/chatglm3-6b-128k",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        64795,
        64797,
        2
      ],
      "stop": [
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "glm4-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-GGUF",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 1048576,
    "model_name": "glm4-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4-9b-chat-1m",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "IQ3_XS",
          "IQ3_S",
          "IQ3_M",
          "Q3_K_S",
          "Q3_K_L",
          "Q3_K",
          "IQ4_XS",
          "IQ4_NL",
          "Q4_K_S",
          "Q4_K",
          "Q5_K_S",
          "Q5_K",
          "Q6_K",
          "Q8_0",
          "BF16",
          "FP16"
        ],
        "model_file_name_template": "glm-4-9b-chat-1m.{quantization}.gguf",
        "model_hub": "modelscope",
        "model_id": "LLM-Research/glm-4-9b-chat-1m-GGUF",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "glm-4v",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "GLM4 is the open source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/glm-4v-9b",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "codegeex4",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "the open-source version of the latest CodeGeeX4 model series",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "ZhipuAI/codegeex4-all-9b",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "IQ2_M",
          "IQ3_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K_L",
          "Q8_0"
        ],
        "model_file_name_template": "codegeex4-all-9b-{quantization}.gguf",
        "model_id": "ZhipuAI/codegeex4-all-9b-GGUF",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "CHATGLM3",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151329,
        151336,
        151338
      ],
      "stop": [
        "<|endoftext|>",
        "<|user|>",
        "<|observation|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "XVERSE-7B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-7B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "xverse/XVERSE-13B-Chat",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "XVERSE",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "xverse",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "XVERSE is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-13B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 65,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "xverse/XVERSE-65B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internlm2.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-1_8b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat-gguf",
        "model_file_name_template": "internlm2_5-7b-chat-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-20b-chat",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "internlm2.5-chat-1m",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "InternLM2.5 series of the InternLM model supports 1M long-context",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2_5-7b-chat-1m",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "wizardcoder-python-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-13B-V1.0",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/WizardCoder-Python-34B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### ",
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama is an open-source LLM trained by fine-tuning LLaMA2 for generating and discussing code.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 8194,
    "model_name": "codeshell-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeShell is a multi-language code LLM developed by the Knowledge Computing Lab of Peking University.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "WisdomShell/CodeShell-7B-Chat",
        "model_revision": "master",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "CodeShell",
      "system_prompt": "",
      "roles": [
        "## human:",
        "## assistant: "
      ],
      "intra_message_sep": "",
      "inter_message_sep": "",
      "stop_token_ids": [
        70000
      ],
      "stop": [
        "<|endoftext|>",
        "|||",
        "|<end>|"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-instruct",
    "model_description": "Code-Llama-Instruct is an instruct-tuned version of the Code-Llama LLM.",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-7b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Instruct-hf",
        "model_revision": "v1.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-34b-Instruct-hf",
        "model_revision": "v1.0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Instruct-GGUF",
        "model_file_name_template": "codellama-7b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Instruct-GGUF",
        "model_file_name_template": "codellama-13b-instruct.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 34,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-34B-Instruct-GGUF",
        "model_file_name_template": "codellama-34b-instruct.{quantization}.gguf",
        "model_revision": "v0.1.0"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "<s>[INST] <<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n<</SYS>>\n\n",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": " </s><s>",
      "stop_token_ids": [
        2
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Mistral-7B is a unmoderated Transformer based LLM claiming to outperform Llama2 on all benchmarks.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_S",
          "Q3_K_M",
          "Q3_K_L",
          "Q4_0",
          "Q4_K_S",
          "Q4_K_M",
          "Q5_0",
          "Q5_K_S",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-v0.1-GGUF",
        "model_file_name_template": "mistral-7b-v0.1.{quantization}.gguf",
        "model_revision": "v1.0.0"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 100000,
    "model_name": "code-llama-python",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Code-Llama-Python is a fine-tuned version of the Code-Llama LLM, specializing in Python.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_id": "Xorbits/CodeLlama-7B-Python-GGUF",
        "model_hub": "modelscope",
        "model_revision": "v1.0.0",
        "model_file_name_template": "codellama-7b-python.{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 13,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_0",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_0",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q8_0"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-GGUF",
        "model_file_name_template": "codellama-13b-python.{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-13B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/CodeLlama-7B-Python-fp16",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/CodeLlama-13b-Python-hf",
        "model_revision": "v1.0.1"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-v0.1",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "mixtral-instruct-v0.1",
    "model_lang": [
      "en",
      "fr",
      "it",
      "de",
      "es"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-8x7B-Instruct is a fine-tuned version of the Mistral-8x7B LLM, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "46_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mixtral-8x7B-Instruct-v0.1",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MIXTRAL_V01",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "",
      "inter_message_sep": ""
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 262144,
    "model_name": "Yi-200k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI. The first public release contains two bilingual (English/Chinese) base models with the parameter sizes of 6B and 34B. Both of them are trained with 4K sequence length and can be extended to 32K during inference time.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-200K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-200K",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Yi series models are large language models trained from scratch by developers at 01.AI.",
    "model_specs": [
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "8bits"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat-{quantization}",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-34B-Chat",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Yi-1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-6B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-GPTQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 6,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-6B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 9,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-9B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_id": "AI-ModelScope/Yi-1.5-34B-Chat-AWQ",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "Yi-1.5-chat-16k",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-9B-Chat-16K",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-1.5-34B-Chat-16K",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "wizardmath-v1.0",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "WizardMath is an open-source LLM trained by fine-tuning Llama2 with Evol-Instruct, specializing in math.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/WizardMath-7B-V1.0",
        "model_revision": "v1.0.0"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE_COT",
      "system_prompt": "Below is an instruction that describes a task. Write a response that appropriately completes the request.",
      "roles": [
        "Instruction",
        "Response"
      ],
      "intra_message_sep": "\n\n### "
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.1",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-7B-Instruct is a fine-tuned version of the Mistral-7B LLM on public datasets, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.1",
        "model_revision": "v1.0.0"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "mistral-instruct-v0.2",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of Mistral-7B-Instruct-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/Mistral-7B-Instruct-v0.2"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Mistral-7B-Instruct-v0.2-GGUF",
        "model_file_name_template": "mistral-7b-instruct-v0.2.{quantization}.gguf"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA2",
      "system_prompt": "[INST] ",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": " ",
      "inter_message_sep": "<s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 1024000,
    "model_name": "mistral-nemo-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Mistral-Nemo-Instruct-2407 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-Nemo-Base-2407",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "none"
        ],
        "model_id": "AI-ModelScope/Mistral-Nemo-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "Int4"
        ],
        "model_id": "LLM-Research/Mistral-Nemo-Instruct-2407-gptq-4bit",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "mistral-nemo",
      "system_prompt": "",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "mistral-large-instruct",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "zh",
      "ru",
      "ja",
      "ko"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Mistral-Large-Instruct-2407 is an advanced dense Large Language Model (LLM) of 123B parameters with state-of-the-art reasoning, knowledge and coding capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "none"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 123,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "LLM-Research/Mistral-Large-Instruct-2407-bnb-4bit",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "mistral-nemo",
      "system_prompt": "",
      "roles": [
        "[INST]",
        "[/INST]"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "</s>",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-alpha",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-α is the first model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "keepitsimple/zephyr-7b-alpha",
        "model_revision": "v1.0-1"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "zephyr-7b-beta",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Zephyr-7B-β is the second model in the series, and is a fine-tuned version of mistralai/Mistral-7B-v0.1",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "modelscope/zephyr-7b-beta",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "<|system|>\nYou are a friendly chatbot.</s>\n",
      "roles": [
        "<|user|>\n",
        "<|assistant|>\n"
      ],
      "intra_message_sep": "</s>\n",
      "inter_message_sep": "</s>\n",
      "stop_token_ids": [
        2
      ],
      "stop": [
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen-chat is a fine-tuned version of the Qwen LLM trained with alignment techniques, specializing in chatting.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-7B-Chat-GGUF",
        "model_file_name_template": "Qwen-7B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "Q4_K_M"
        ],
        "model_hub": "modelscope",
        "model_id": "Xorbits/Qwen-14B-Chat-GGUF",
        "model_file_name_template": "Qwen-14B-Chat.{quantization}.gguf",
        "model_revision": "v0.0.1"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-1_8B-Chat",
        "model_revision": "v1.0.0"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-7B-Chat",
        "model_revision": "v1.1.9"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "qwen/Qwen-72B-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen-14B-Chat",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-1_8B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-7B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.1.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-14B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "v1.0.7"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen-72B-Chat-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 32,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 110,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 4,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 32,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 110,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-110B-Chat-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-0.5B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-0_5b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_8",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-1.8B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-1_8b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 4,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-4B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-4b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-14B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-14b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 32,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/Qwen1.5-32B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-32b-chat-{quantization}.gguf"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_k_m"
        ],
        "model_id": "qwen/Qwen1.5-72B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen1_5-72b-chat-{quantization}.gguf",
        "model_file_name_split_template": "qwen1_5-72b-chat-{quantization}.gguf.{part}",
        "quantization_parts": {
          "q4_k_m": [
            "a",
            "b"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen1.5-moe-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen1.5-MoE is a transformer-based MoE decoder-only language model pretrained on a large amount of data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "2_7",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 65536,
    "model_name": "codeqwen1.5-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "CodeQwen1.5 is the Code-Specific version of Qwen1.5. It is a transformer-based decoder-only language model pretrained on a large amount of data of codes.",
    "model_specs": [
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "codeqwen-1_5-7b-chat-{quantization}.gguf"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/CodeQwen1.5-7B-Chat-AWQ",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 72,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4",
          "Int8"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GPTQ-{quantization}",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 72,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-AWQ",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 7,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-7B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "fp8",
        "model_size_in_billions": 72,
        "quantizations": [
          "fp8"
        ],
        "model_id": "liuzhenghua/Qwen2-72B-FP8-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "mlx",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-MLX",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "0_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-0.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-0_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": "1_5",
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-1.5B-Instruct-GGUF",
        "model_file_name_template": "qwen2-1_5b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 7,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-7B-Instruct-GGUF",
        "model_file_name_template": "qwen2-7b-instruct-{quantization}.gguf",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 72,
        "quantizations": [
          "q2_k",
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-72B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-72b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-72b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q5_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q5_k_m": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q6_k": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00004",
            "00002-of-00004",
            "00003-of-00004",
            "00004-of-00004"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "qwen2-moe-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "tools"
    ],
    "model_description": "Qwen2 is the new series of Qwen large language models. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 14,
        "quantizations": [
          "q3_k_m",
          "q4_0",
          "q4_k_m",
          "q5_0",
          "q5_k_m",
          "q6_k",
          "q8_0",
          "fp16"
        ],
        "model_id": "qwen/Qwen2-57B-A14B-Instruct-GGUF",
        "model_hub": "modelscope",
        "model_file_name_template": "qwen2-57b-a14b-instruct-{quantization}.gguf",
        "model_file_name_split_template": "qwen2-57b-a14b-instruct-{quantization}-{part}.gguf",
        "quantization_parts": {
          "q8_0": [
            "00001-of-00002",
            "00002-of-00002"
          ],
          "fp16": [
            "00001-of-00003",
            "00002-of-00003",
            "00003-of-00003"
          ]
        }
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-1.3b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-vl-7b-chat",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CHAT",
      "system_prompt": "<｜begin▁of▁sentence｜>",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<｜end▁of▁sentence｜>",
      "stop": [
        "<｜end▁of▁sentence｜>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "DDeepSeek LLM, trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. ",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "deepseek-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "DeepSeek LLM is an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-7b-chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 67,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-llm-67b-chat",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CHAT",
      "system_prompt": "<｜begin▁of▁sentence｜>",
      "roles": [
        "User",
        "Assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<｜end▁of▁sentence｜>",
      "stop": [
        "<｜end▁of▁sentence｜>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-base",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-base",
        "model_hub": "modelscope"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "deepseek-coder-instruct",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "deepseek-coder-instruct is a model initialized from deepseek-coder-base and fine-tuned on 2B tokens of instruction data.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": "1_3",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": "6_7",
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 33,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "deepseek-ai/deepseek-coder-33b-instruct",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "DEEPSEEK_CODER",
      "system_prompt": "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.",
      "roles": [
        "### Instruction:",
        "### Response:"
      ],
      "inter_message_sep": "\n",
      "stop": [
        "<|EOT|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-base",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "Skywork-Math",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Skywork is a series of large models developed by the Kunlun Group · Skywork team.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 13,
        "quantizations": [
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "skywork/Skywork-13B-Math",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 204800,
    "model_name": "internlm2-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The second generation of the InternLM model, InternLM2.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2-chat-7b",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_id": "Shanghai_AI_Laboratory/internlm2-chat-20b",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "INTERNLM2",
      "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "stop_token_ids": [
        2,
        92542
      ],
      "stop": [
        "</s>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "qwen-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Qwen-VL-Chat supports more flexible interaction, such as multiple image inputs, multi-round question answering, and creative capabilities.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "Qwen/Qwen-VL-Chat-{quantization}",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "QWEN",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        151643,
        151644,
        151645
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "OrionStarAI/Orion-14B-Chat",
        "model_hub": "modelscope"
      },
      {
        "model_format": "awq",
        "model_size_in_billions": 14,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "OrionStarAI/Orion-14B-Chat-{quantization}"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "orion-chat-rag",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Orion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 14,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "OrionStarAI/Orion-14B-Chat-RAG"
      }
    ],
    "prompt_style": {
      "style_name": "orion",
      "roles": [
        "Human",
        "assistant"
      ],
      "stop": [
        "<s>",
        "</s>",
        "<unk>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "yi-vl-chat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat",
      "vision"
    ],
    "model_description": "Yi Vision Language (Yi-VL) model is the open-source, multimodal version of the Yi Large Language Model (LLM) series, enabling content comprehension, recognition, and multi-round conversations about images.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 6,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-VL-6B"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "01ai/Yi-VL-34B"
      }
    ],
    "prompt_style": {
      "style_name": "CHATML",
      "system_prompt": "",
      "roles": [
        "<|im_start|>user",
        "<|im_start|>assistant"
      ],
      "intra_message_sep": "<|im_end|>",
      "inter_message_sep": "",
      "stop_token_ids": [
        2,
        6,
        7,
        8
      ],
      "stop": [
        "<|endoftext|>",
        "<|im_start|>",
        "<|im_end|>",
        "<|im_sep|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/gemma-2b-it"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/gemma-7b-it"
      }
    ],
    "prompt_style": {
      "style_name": "gemma",
      "roles": [
        "user",
        "model"
      ],
      "stop": [
        "<end_of_turn>",
        "<start_of_turn>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "gemma-2-it",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "LLM-Research/gemma-2-2b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 9,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "AI-ModelScope/gemma-2-9b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 27,
        "quantizations": [
          "none",
          "4-bit",
          "8-bit"
        ],
        "model_id": "AI-ModelScope/gemma-2-27b-it",
        "model_hub": "modelscope"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 9,
        "quantizations": [
          "Q2_K",
          "Q3_K_L",
          "Q3_K_M",
          "Q3_K_S",
          "Q4_K_L",
          "Q4_K_M",
          "Q4_K_S",
          "Q5_K_L",
          "Q5_K_M",
          "Q5_K_S",
          "Q6_K",
          "Q6_K_L",
          "Q8_0",
          "f32"
        ],
        "model_id": "LLM-Research/gemma-2-9b-it-GGUF",
        "model_file_name_template": "gemma-2-9b-it-{quantization}.gguf",
        "model_hub": "modelscope"
      }
    ],
    "prompt_style": {
      "style_name": "gemma",
      "roles": [
        "user",
        "model"
      ],
      "stop": [
        "<end_of_turn>",
        "<start_of_turn>"
      ]
    }
  },
  {
    "version":1,
    "context_length":2048,
    "model_name":"OmniLMM",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"OmniLMM is a family of open-source large multimodal models (LMMs) adept at vision & language modeling.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":3,
        "quantizations":[
          "none"
        ],
        "model_id":"OpenBMB/MiniCPM-V",
        "model_hub":"modelscope",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":12,
        "quantizations":[
          "none"
        ],
        "model_id":"OpenBMB/OmniLMM-12B",
        "model_hub":"modelscope",
        "model_revision":"master"
      }
    ],
    "prompt_style":{
      "style_name":"OmniLMM",
      "system_prompt":"The role of first msg should be user",
      "roles":[
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/miniCPM-bf16",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-sft-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-sft-fp32",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-bf16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-bf16",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp16",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp16",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "minicpm-2b-dpo-fp32",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 2,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "OpenBMB/MiniCPM-2B-dpo-fp32",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "MINICPM-2B",
      "system_prompt": "",
      "roles": [
        "user",
        "assistant"
      ],
      "stop_token_ids": [
        1,
        2
      ],
      "stop": [
        "<s>",
        "</s>"
      ]
    }
  },
  {
    "version":1,
    "context_length":8192,
    "model_name":"MiniCPM-Llama3-V-2_5",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-Llama3-V 2.5 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Llama3-8B-Instruct with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-Llama3-V-2_5",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "int4"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-Llama3-V-2_5-{quantization}",
        "model_revision":"master"
      }
    ],
    "prompt_style":{
      "style_name":"OmniLMM",
      "system_prompt":"The role of first msg should be user",
      "roles":[
        "user",
        "assistant"
      ]
    }
  },
  {
    "version":1,
    "context_length":32768,
    "model_name":"MiniCPM-V-2.6",
    "model_lang":[
      "en",
      "zh"
    ],
    "model_ability":[
      "chat",
      "vision"
    ],
    "model_description":"MiniCPM-V 2.6 is the latest model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters.",
    "model_specs":[
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "none"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-V-2_6",
        "model_revision":"master"
      },
      {
        "model_format":"pytorch",
        "model_size_in_billions":8,
        "quantizations":[
          "4-bit"
        ],
        "model_hub": "modelscope",
        "model_id":"OpenBMB/MiniCPM-V-2_6-int4",
        "model_revision":"master"
      }
    ],
    "prompt_style":{
      "style_name":"QWEN",
      "system_prompt":"You are a helpful assistant",
      "roles":[
        "user",
        "assistant"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "Aquila2 series models are the base language models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/Aquila2-34B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/Aquila2-70B-Expr",
        "model_revision": "master"
      }
    ]
  },
  {
    "version": 1,
    "context_length": 2048,
    "model_name": "aquila2-chat",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "Aquila2-chat series models are the chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 34,
        "quantizations": [
          "Int4"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B-Int4-GPTQ",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 70,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-70B-Expr",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "intra_message_sep": "\n",
      "system_prompt": "",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "stop_token_ids": [
        100006,
        100007
      ],
      "stop": [
        "[CLS]",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 16384,
    "model_name": "aquila2-chat-16k",
    "model_lang": [
      "zh"
    ],
    "model_ability": [
      "generate"
    ],
    "model_description": "AquilaChat2-16k series models are the long-text chat models",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 34,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "BAAI/AquilaChat2-34B-16K",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "ADD_COLON_SINGLE",
      "intra_message_sep": "\n",
      "system_prompt": "",
      "roles": [
        "USER",
        "ASSISTANT"
      ],
      "stop_token_ids": [
        100006,
        100007
      ],
      "stop": [
        "[CLS]",
        "</s>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 131072,
    "model_name": "c4ai-command-r-v01",
    "model_lang": [
      "en",
      "fr",
      "de",
      "es",
      "it",
      "pt",
      "ja",
      "ko",
      "zh",
      "ar"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "C4AI Command-R is a research release of a 35 billion parameter highly performant generative model.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/c4ai-command-r-v01",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 35,
        "quantizations": [
          "4-bit"
        ],
        "model_hub": "modelscope",
        "model_id": "mirror013/c4ai-command-r-v01-4bit",
        "model_revision": "master"
      },
      {
        "model_format": "ggufv2",
        "model_size_in_billions": 35,
        "quantizations": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M"
        ],
        "model_id": "mirror013/C4AI-Command-R-v01-GGUF",
        "model_file_name_template": "c4ai-command-r-v01-{quantization}.gguf",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 104,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "AI-ModelScope/c4ai-command-r-plus",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "c4ai-command-r",
      "system_prompt": "You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.",
      "roles": [
        "<|USER_TOKEN|>",
        "<|CHATBOT_TOKEN|>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|>",
      "stop_token_ids": [
        6,
        255001
      ]
    }
  },
  {
    "version": 1,
    "context_length": 128000,
    "model_name": "phi-3-mini-128k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-128K-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-128k-instruct",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "PHI3",
      "system_prompt": "You are a helpful AI assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "<|end|>\n",
      "stop_token_ids":[
        32000,
        32007
      ],
      "stop": [
        "<|endoftext|>",
        "<|end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 4096,
    "model_name": "phi-3-mini-4k-instruct",
    "model_lang": [
      "en"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The Phi-3-Mini-4k-Instruct is a 3.8 billion-parameter, lightweight, state-of-the-art open model trained using the Phi-3 datasets.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 4,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "LLM-Research/Phi-3-mini-4k-instruct",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "PHI3",
      "system_prompt": "You are a helpful AI assistant.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n",
      "inter_message_sep": "<|end|>\n",
      "stop_token_ids":[
        32000,
        32007
      ],
      "stop": [
        "<|endoftext|>",
        "<|end|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl-chat",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 1.5 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [
        {
            "model_format": "pytorch",
            "model_size_in_billions": 26,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL-Chat-V1-5",
            "model_revision": "master"
        }
    ],
    "prompt_style": {
        "style_name": "INTERNVL",
        "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
        "roles": [
            "<|im_start|>user",
            "<|im_start|>assistant"
        ],
        "intra_message_sep": "<|im_end|>",
        "stop_token_ids": [
            2,
            92543,
            92542
        ],
        "stop": [
            "</s>",
            "<|im_end|>",
            "<|im_start|>"
        ]
    }
  },
  {
    "version": 1,
    "context_length": 32768,
    "model_name": "internvl2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "InternVL 2 is an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. ",
    "model_specs": [

        {
            "model_format": "pytorch",
            "model_size_in_billions": 1,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-1B",
            "model_revision": "master"
        },
      {
            "model_format": "pytorch",
            "model_size_in_billions": 2,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-2B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 2,
            "quantizations": [
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-2B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 4,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-4B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 8,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-8B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 8,
            "quantizations": [
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-8B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 26,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-26B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 26,
            "quantizations": [
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-26B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 40,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-40B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 40,
            "quantizations": [
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-40B-AWQ",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 76,
            "quantizations": [
              "4-bit",
              "8-bit",
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-Llama3-76B",
            "model_revision": "master"
        },
        {
            "model_format": "pytorch",
            "model_size_in_billions": 76,
            "quantizations": [
              "none"
            ],
            "model_hub": "modelscope",
            "model_id": "OpenGVLab/InternVL2-Llama3-76B-AWQ",
            "model_revision": "master"
        }
    ],
    "prompt_style": {
        "style_name": "INTERNVL",
        "system_prompt": "You are InternLM (书生·浦语), a helpful, honest, and harmless AI assistant developed by Shanghai AI Laboratory (上海人工智能实验室).",
        "roles": [
            "<|im_start|>user",
            "<|im_start|>assistant"
        ],
        "intra_message_sep": "<|im_end|>",
        "stop_token_ids": [
            2,
            92543,
            92542
        ],
        "stop": [
            "</s>",
            "<|im_end|>",
            "<|im_start|>"
        ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "cogvlm2",
    "model_lang": [
        "en",
        "zh"
    ],
    "model_ability": [
        "chat",
        "vision"
    ],
    "model_description": "CogVLM2 have achieved good results in many lists compared to the previous generation of CogVLM open source models. Its excellent performance can compete with some non-open source models.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "none"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 20,
        "quantizations": [
          "int4"
        ],
        "model_hub": "modelscope",
        "model_id": "ZhipuAI/cogvlm2-llama3-chinese-chat-19B-{quantization}",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "LLAMA3",
      "system_prompt": "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.",
      "roles": [
        "user",
        "assistant"
      ],
      "intra_message_sep": "\n\n",
      "inter_message_sep": "<|eot_id|>",
      "stop_token_ids": [
        128001,
        128009
      ],
      "stop": [
        "<|end_of_text|>",
        "<|eot_id|>"
      ]
    }
  },
  {
    "version": 1,
    "context_length": 8192,
    "model_name": "telechat",
    "model_lang": [
      "en",
      "zh"
    ],
    "model_ability": [
      "chat"
    ],
    "model_description": "The TeleChat is a large language model developed and trained by China Telecom Artificial Intelligence Technology Co., LTD. The 7B model base is trained with 1.5 trillion Tokens and 3 trillion Tokens and Chinese high-quality corpus.",
    "model_specs": [
      {
        "model_format": "pytorch",
        "model_size_in_billions": 7,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/telechat-7B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 7,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/telechat-7B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 12,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/TeleChat-12B",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "gptq",
        "model_size_in_billions": 12,
        "quantizations": [
          "int4",
          "int8"
        ],
        "model_id": "TeleAI/TeleChat-12B-{quantization}",
        "model_hub": "modelscope",
        "model_revision": "master"
      },
      {
        "model_format": "pytorch",
        "model_size_in_billions": 52,
        "quantizations": [
          "4-bit",
          "8-bit",
          "none"
        ],
        "model_id": "TeleAI/TeleChat-52B",
        "model_hub": "modelscope",
        "model_revision": "master"
      }
    ],
    "prompt_style": {
      "style_name": "NO_COLON_TWO",
      "system_prompt": "You are a helpful assistant.",
      "roles": [
        "<_user>",
        "<_bot>"
      ],
      "intra_message_sep": "",
      "inter_message_sep": "",
      "stop": [
        "<_end>",
        "<_start>"
      ],
      "stop_token_ids": [
        160133,
        160132
      ]
    }
  }
]
